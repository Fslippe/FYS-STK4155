\relax 
\providecommand\hyper@newdestlabel[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{nty/global//global/global}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\babel@aux{english}{}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2}Method}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Splitting and scaling of data}{6}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Mean squared error and R squared}{7}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Bias-variance trade-off}{7}{subsection.2.3}\protected@file@percent }
\abx@aux@cite{cv}
\abx@aux@segm{0}{0}{cv}
\abx@aux@cite{cv}
\abx@aux@segm{0}{0}{cv}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Cross validation}{9}{subsection.2.4}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A visualization of the splits of the data performed in cross validation. Here we see 4 $k$-folds which gives a total of 4 splits and iterations. \cite {cv}\relax }}{9}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:k_fold}{{1}{9}{A visualization of the splits of the data performed in cross validation. Here we see 4 $k$-folds which gives a total of 4 splits and iterations. \cite {cv}\relax }{figure.caption.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Study of $\lambda $ dependence for ridge and lasso regression}{10}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Study of topography data}{10}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{11}{section.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Franke funtion}{11}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}OLS scaling of data}{11}{subsubsection.3.1.1}\protected@file@percent }
\newlabel{fig:}{{2a}{11}{$MSE$\relax }{figure.caption.3}{}}
\newlabel{sub@fig:}{{a}{11}{$MSE$\relax }{figure.caption.3}{}}
\newlabel{fig:}{{2b}{11}{$R^2$\relax }{figure.caption.3}{}}
\newlabel{sub@fig:}{{b}{11}{$R^2$\relax }{figure.caption.3}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison of MSE and $R^2$ between scaled and non-scaled data\relax }}{11}{figure.caption.3}\protected@file@percent }
\newlabel{fig:compare_scale}{{2}{11}{Comparison of MSE and $R^2$ between scaled and non-scaled data\relax }{figure.caption.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}OLS MSE and $R^2$}{11}{subsubsection.3.1.2}\protected@file@percent }
\newlabel{fig:}{{3a}{12}{$MSE$\relax }{figure.caption.4}{}}
\newlabel{sub@fig:}{{a}{12}{$MSE$\relax }{figure.caption.4}{}}
\newlabel{fig:}{{3b}{12}{$R^2$\relax }{figure.caption.4}{}}
\newlabel{sub@fig:}{{b}{12}{$R^2$\relax }{figure.caption.4}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces plots of MSE and $R^2$ for the franke function for an error $\epsilon \sim N(0, \sigma ^2)$ with $\sigma =0.2$ and 31 points in each direction\relax }}{12}{figure.caption.4}\protected@file@percent }
\newlabel{fig:r2_mse_5}{{3}{12}{plots of MSE and $R^2$ for the franke function for an error $\epsilon \sim N(0, \sigma ^2)$ with $\sigma =0.2$ and 31 points in each direction\relax }{figure.caption.4}{}}
\newlabel{fig:}{{4a}{12}{$MSE$\relax }{figure.caption.5}{}}
\newlabel{sub@fig:}{{a}{12}{$MSE$\relax }{figure.caption.5}{}}
\newlabel{fig:}{{4b}{12}{$R^2$\relax }{figure.caption.5}{}}
\newlabel{sub@fig:}{{b}{12}{$R^2$\relax }{figure.caption.5}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces MSE and $R^2$ computed from both the train and test sample\relax }}{12}{figure.caption.5}\protected@file@percent }
\newlabel{fig:train_test}{{4}{12}{MSE and $R^2$ computed from both the train and test sample\relax }{figure.caption.5}{}}
\newlabel{fig:train_test_resample_mse}{{5a}{13}{$MSE$\relax }{figure.caption.6}{}}
\newlabel{sub@fig:train_test_resample_mse}{{a}{13}{$MSE$\relax }{figure.caption.6}{}}
\newlabel{fig:train_test_resample_r2}{{5b}{13}{$R^2$\relax }{figure.caption.6}{}}
\newlabel{sub@fig:train_test_resample_r2}{{b}{13}{$R^2$\relax }{figure.caption.6}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces An average of MSE and $R^2$ computed from both the train and test sample over 100 unique samples\relax }}{13}{figure.caption.6}\protected@file@percent }
\newlabel{fig:train_test_resample}{{5}{13}{An average of MSE and $R^2$ computed from both the train and test sample over 100 unique samples\relax }{figure.caption.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}OLS variation of $\beta $ paramers}{13}{subsubsection.3.1.3}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \relax }}{14}{figure.caption.7}\protected@file@percent }
\newlabel{fig:beta}{{6}{14}{\relax }{figure.caption.7}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Variance of the $\beta $ parameters for test and train data for a design matrix of polynomial degree 5. Calculated by $Var(\boldsymbol  {\beta }) = diag(\sigma ^2(X^T X)^{-1}$)\relax }}{15}{table.caption.8}\protected@file@percent }
\newlabel{tab:beta}{{1}{15}{Variance of the $\beta $ parameters for test and train data for a design matrix of polynomial degree 5. Calculated by $Var(\boldsymbol {\beta }) = diag(\sigma ^2(X^T X)^{-1}$)\relax }{table.caption.8}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}OLS bias variance tradeoff}{15}{subsubsection.3.1.4}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The bias variance tradeoff for the Franke function using 100 bootstrap iteration together with $n=30$ steps and a noise standard deviation of $\sigma =0.2$\relax }}{16}{figure.caption.9}\protected@file@percent }
\newlabel{fig:bias_variance}{{7}{16}{The bias variance tradeoff for the Franke function using 100 bootstrap iteration together with $n=30$ steps and a noise standard deviation of $\sigma =0.2$\relax }{figure.caption.9}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The bias variance tradeoff for the Franke function using 100 bootstrap iteration together with $n=100$ steps and a noise standard deviation of $\sigma =0.2$\relax }}{17}{figure.caption.10}\protected@file@percent }
\newlabel{fig:bias_variance_100}{{8}{17}{The bias variance tradeoff for the Franke function using 100 bootstrap iteration together with $n=100$ steps and a noise standard deviation of $\sigma =0.2$\relax }{figure.caption.10}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.5}OLS cross validation}{17}{subsubsection.3.1.5}\protected@file@percent }
\newlabel{fig:cv_b_comp_5}{{9a}{17}{5 $k$-folds\relax }{figure.caption.11}{}}
\newlabel{sub@fig:cv_b_comp_5}{{a}{17}{5 $k$-folds\relax }{figure.caption.11}{}}
\newlabel{fig:cv_b_comp_10}{{9b}{17}{10 $k$-folds\relax }{figure.caption.11}{}}
\newlabel{sub@fig:cv_b_comp_10}{{b}{17}{10 $k$-folds\relax }{figure.caption.11}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces A comparison between bootstrap and cross validation for datasets with $n=30$ steps, noise standard deviation $\sigma =0.2$ and 100 bootstrap iterations\relax }}{17}{figure.caption.11}\protected@file@percent }
\newlabel{fig:cv_comp}{{9}{17}{A comparison between bootstrap and cross validation for datasets with $n=30$ steps, noise standard deviation $\sigma =0.2$ and 100 bootstrap iterations\relax }{figure.caption.11}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces A comparison between bootstrap and cross validation for datasets with $n=30$ steps, noise standard deviation $\sigma =0.1$ and 100 bootstrap iterations\relax }}{18}{figure.caption.12}\protected@file@percent }
\newlabel{fig:cv_comp_01}{{10}{18}{A comparison between bootstrap and cross validation for datasets with $n=30$ steps, noise standard deviation $\sigma =0.1$ and 100 bootstrap iterations\relax }{figure.caption.12}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Cross validation to find which polynomial degree gives the lowest MSE for data with $n=30$ steps and noise standard deviation $\sigma =0.2$\relax }}{19}{figure.caption.13}\protected@file@percent }
\newlabel{fig:best_OLS}{{11}{19}{Cross validation to find which polynomial degree gives the lowest MSE for data with $n=30$ steps and noise standard deviation $\sigma =0.2$\relax }{figure.caption.13}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.6}Ridge and Lasso bias variance tradeoff}{19}{subsubsection.3.1.6}\protected@file@percent }
\newlabel{fig:}{{12a}{20}{Ridge\relax }{figure.caption.14}{}}
\newlabel{sub@fig:}{{a}{20}{Ridge\relax }{figure.caption.14}{}}
\newlabel{fig:}{{12b}{20}{Lasso\relax }{figure.caption.14}{}}
\newlabel{sub@fig:}{{b}{20}{Lasso\relax }{figure.caption.14}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces A comparison on how $\lambda $ influences the $\beta $ values between Ridge and Lasso regression\relax }}{20}{figure.caption.14}\protected@file@percent }
\newlabel{fig:beta_lambda_rl}{{12}{20}{A comparison on how $\lambda $ influences the $\beta $ values between Ridge and Lasso regression\relax }{figure.caption.14}{}}
\newlabel{fig:l_1e-07}{{13a}{21}{\relax }{figure.caption.15}{}}
\newlabel{sub@fig:l_1e-07}{{a}{21}{\relax }{figure.caption.15}{}}
\newlabel{fig:l_1e-08}{{13b}{21}{\relax }{figure.caption.15}{}}
\newlabel{sub@fig:l_1e-08}{{b}{21}{\relax }{figure.caption.15}{}}
\newlabel{fig:l_1e-09}{{13c}{21}{\relax }{figure.caption.15}{}}
\newlabel{sub@fig:l_1e-09}{{c}{21}{\relax }{figure.caption.15}{}}
\newlabel{fig:l_1e-10}{{13d}{21}{\relax }{figure.caption.15}{}}
\newlabel{sub@fig:l_1e-10}{{d}{21}{\relax }{figure.caption.15}{}}
\newlabel{fig:l_1e-11}{{13e}{21}{\relax }{figure.caption.15}{}}
\newlabel{sub@fig:l_1e-11}{{e}{21}{\relax }{figure.caption.15}{}}
\newlabel{fig:l_1e-12}{{13f}{21}{\relax }{figure.caption.15}{}}
\newlabel{sub@fig:l_1e-12}{{f}{21}{\relax }{figure.caption.15}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Bias variance tradeoff for different choices of lambda using Ridge regression for data with $n=30$ steps and noise standard deviation of $\sigma =0.2$\relax }}{21}{figure.caption.15}\protected@file@percent }
\newlabel{fig:ridge_tradeoff}{{13}{21}{Bias variance tradeoff for different choices of lambda using Ridge regression for data with $n=30$ steps and noise standard deviation of $\sigma =0.2$\relax }{figure.caption.15}{}}
\newlabel{fig:l_1e-08}{{14a}{22}{\relax }{figure.caption.16}{}}
\newlabel{sub@fig:l_1e-08}{{a}{22}{\relax }{figure.caption.16}{}}
\newlabel{fig:l_1e-09}{{14b}{22}{\relax }{figure.caption.16}{}}
\newlabel{sub@fig:l_1e-09}{{b}{22}{\relax }{figure.caption.16}{}}
\newlabel{fig:l_1e-10}{{14c}{22}{\relax }{figure.caption.16}{}}
\newlabel{sub@fig:l_1e-10}{{c}{22}{\relax }{figure.caption.16}{}}
\newlabel{fig:l_1e-1}{{14d}{22}{\relax }{figure.caption.16}{}}
\newlabel{sub@fig:l_1e-1}{{d}{22}{\relax }{figure.caption.16}{}}
\newlabel{fig:l_1e-12}{{14e}{22}{\relax }{figure.caption.16}{}}
\newlabel{sub@fig:l_1e-12}{{e}{22}{\relax }{figure.caption.16}{}}
\newlabel{fig:l_1e-13}{{14f}{22}{\relax }{figure.caption.16}{}}
\newlabel{sub@fig:l_1e-13}{{f}{22}{\relax }{figure.caption.16}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Bias variance tradeoff for different choices of lambda using Lasso regression for data with $n=30$ steps and noise standard deviation of $\sigma =0.2$\relax }}{22}{figure.caption.16}\protected@file@percent }
\newlabel{fig:lasso_tradeoff}{{14}{22}{Bias variance tradeoff for different choices of lambda using Lasso regression for data with $n=30$ steps and noise standard deviation of $\sigma =0.2$\relax }{figure.caption.16}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.7}Finding best models}{23}{subsubsection.3.1.7}\protected@file@percent }
\newlabel{fig:}{{15a}{23}{Ridge regression\relax }{figure.caption.17}{}}
\newlabel{sub@fig:}{{a}{23}{Ridge regression\relax }{figure.caption.17}{}}
\newlabel{fig:}{{15b}{23}{Lasso regression\relax }{figure.caption.17}{}}
\newlabel{sub@fig:}{{b}{23}{Lasso regression\relax }{figure.caption.17}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Heatmap of MSE for different choices of polynomial degree and $\lambda $. Here we have used cross validation with 5$k$-folds for data with $n=30$ steps and noise standard deviation of $\sigma =0.2$\relax }}{23}{figure.caption.17}\protected@file@percent }
\newlabel{fig:heat_franke}{{15}{23}{Heatmap of MSE for different choices of polynomial degree and $\lambda $. Here we have used cross validation with 5$k$-folds for data with $n=30$ steps and noise standard deviation of $\sigma =0.2$\relax }{figure.caption.17}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Table of best parameters and the following MSE for OLS, Ridge and Lasso regression\relax }}{24}{table.caption.18}\protected@file@percent }
\newlabel{tab:best_comp}{{2}{24}{Table of best parameters and the following MSE for OLS, Ridge and Lasso regression\relax }{table.caption.18}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.8}Prediction plots}{24}{subsubsection.3.1.8}\protected@file@percent }
\newlabel{fig:pred_real}{{16a}{25}{Franke function\relax }{figure.caption.19}{}}
\newlabel{sub@fig:pred_real}{{a}{25}{Franke function\relax }{figure.caption.19}{}}
\newlabel{fig:pred_train}{{16b}{25}{Train data from function data with added noise\relax }{figure.caption.19}{}}
\newlabel{sub@fig:pred_train}{{b}{25}{Train data from function data with added noise\relax }{figure.caption.19}{}}
\newlabel{fig:pred_test}{{16c}{25}{Test data from function data with added noise\relax }{figure.caption.19}{}}
\newlabel{sub@fig:pred_test}{{c}{25}{Test data from function data with added noise\relax }{figure.caption.19}{}}
\newlabel{fig:pred_ols}{{16d}{25}{OLS prediction\relax }{figure.caption.19}{}}
\newlabel{sub@fig:pred_ols}{{d}{25}{OLS prediction\relax }{figure.caption.19}{}}
\newlabel{fig:pred_ridge}{{16e}{25}{Ridge prediction\relax }{figure.caption.19}{}}
\newlabel{sub@fig:pred_ridge}{{e}{25}{Ridge prediction\relax }{figure.caption.19}{}}
\newlabel{fig:pred_lasso}{{16f}{25}{Lasso prediction\relax }{figure.caption.19}{}}
\newlabel{sub@fig:pred_lasso}{{f}{25}{Lasso prediction\relax }{figure.caption.19}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Plot of the true Franke function together with our train and test data, and our predictions for our optimal parameters shown in table \ref  {tab:best_comp} \relax }}{25}{figure.caption.19}\protected@file@percent }
\newlabel{fig:pred_franke}{{16}{25}{Plot of the true Franke function together with our train and test data, and our predictions for our optimal parameters shown in table \ref {tab:best_comp} \relax }{figure.caption.19}{}}
\newlabel{fig:extra_pred_real}{{17a}{27}{Franke function\relax }{figure.caption.20}{}}
\newlabel{sub@fig:extra_pred_real}{{a}{27}{Franke function\relax }{figure.caption.20}{}}
\newlabel{fig:extra_pred_train}{{17b}{27}{Train data from function data with added noise\relax }{figure.caption.20}{}}
\newlabel{sub@fig:extra_pred_train}{{b}{27}{Train data from function data with added noise\relax }{figure.caption.20}{}}
\newlabel{fig:extra_pred_test}{{17c}{27}{Test data from function data with added noise\relax }{figure.caption.20}{}}
\newlabel{sub@fig:extra_pred_test}{{c}{27}{Test data from function data with added noise\relax }{figure.caption.20}{}}
\newlabel{fig:extra_pred_ols}{{17d}{27}{OLS prediction\relax }{figure.caption.20}{}}
\newlabel{sub@fig:extra_pred_ols}{{d}{27}{OLS prediction\relax }{figure.caption.20}{}}
\newlabel{fig:extra_pred_ridge}{{17e}{27}{Ridge prediction\relax }{figure.caption.20}{}}
\newlabel{sub@fig:extra_pred_ridge}{{e}{27}{Ridge prediction\relax }{figure.caption.20}{}}
\newlabel{fig:extra_pred_lasso}{{17f}{27}{Lasso prediction\relax }{figure.caption.20}{}}
\newlabel{sub@fig:extra_pred_lasso}{{f}{27}{Lasso prediction\relax }{figure.caption.20}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Plot of the true Franke function together with our train and test data, and our predictions for our optimal parameters shown in table \ref  {tab:best_comp} using $n=200$ steps and $\sigma =0.2$ \relax }}{27}{figure.caption.20}\protected@file@percent }
\newlabel{fig:extra_pred_franke}{{17}{27}{Plot of the true Franke function together with our train and test data, and our predictions for our optimal parameters shown in table \ref {tab:best_comp} using $n=200$ steps and $\sigma =0.2$ \relax }{figure.caption.20}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Topography data}{28}{subsection.3.2}\protected@file@percent }
\newlabel{fig:}{{18a}{28}{\relax }{figure.caption.21}{}}
\newlabel{sub@fig:}{{a}{28}{\relax }{figure.caption.21}{}}
\newlabel{fig:}{{18b}{28}{\relax }{figure.caption.21}{}}
\newlabel{sub@fig:}{{b}{28}{\relax }{figure.caption.21}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces MSE and $R^2$ as a function of polynomial degree for OLS predction. Here we are using indexes of [100:141] where every second index are skipped.\relax }}{28}{figure.caption.21}\protected@file@percent }
\newlabel{fig:mse_r2_real}{{18}{28}{MSE and $R^2$ as a function of polynomial degree for OLS predction. Here we are using indexes of [100:141] where every second index are skipped.\relax }{figure.caption.21}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Bias variance tradeoff}{28}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Bias variance tradeoff for the OLS method\relax }}{29}{figure.caption.22}\protected@file@percent }
\newlabel{fig:tradeoff_real}{{19}{29}{Bias variance tradeoff for the OLS method\relax }{figure.caption.22}{}}
\newlabel{fig:}{{20a}{29}{\relax }{figure.caption.23}{}}
\newlabel{sub@fig:}{{a}{29}{\relax }{figure.caption.23}{}}
\newlabel{fig:}{{20b}{29}{\relax }{figure.caption.23}{}}
\newlabel{sub@fig:}{{b}{29}{\relax }{figure.caption.23}{}}
\newlabel{fig:}{{20c}{29}{\relax }{figure.caption.23}{}}
\newlabel{sub@fig:}{{c}{29}{\relax }{figure.caption.23}{}}
\newlabel{fig:}{{20d}{29}{\relax }{figure.caption.23}{}}
\newlabel{sub@fig:}{{d}{29}{\relax }{figure.caption.23}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Bias variance tradeoff for different choices of lambda using Ridge regression\relax }}{29}{figure.caption.23}\protected@file@percent }
\newlabel{fig:ridge_tradeoff_real}{{20}{29}{Bias variance tradeoff for different choices of lambda using Ridge regression\relax }{figure.caption.23}{}}
\newlabel{fig:}{{21a}{30}{\relax }{figure.caption.24}{}}
\newlabel{sub@fig:}{{a}{30}{\relax }{figure.caption.24}{}}
\newlabel{fig:}{{21b}{30}{\relax }{figure.caption.24}{}}
\newlabel{sub@fig:}{{b}{30}{\relax }{figure.caption.24}{}}
\newlabel{fig:}{{21c}{30}{\relax }{figure.caption.24}{}}
\newlabel{sub@fig:}{{c}{30}{\relax }{figure.caption.24}{}}
\newlabel{fig:}{{21d}{30}{\relax }{figure.caption.24}{}}
\newlabel{sub@fig:}{{d}{30}{\relax }{figure.caption.24}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Bias variance tradeoff for different choices of lambda using Ridge regression\relax }}{30}{figure.caption.24}\protected@file@percent }
\newlabel{fig:lasso_tradeoff_real}{{21}{30}{Bias variance tradeoff for different choices of lambda using Ridge regression\relax }{figure.caption.24}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Finding best models}{31}{subsection.3.3}\protected@file@percent }
\newlabel{fig:best_ols_real}{{22a}{31}{OLS\relax }{figure.caption.25}{}}
\newlabel{sub@fig:best_ols_real}{{a}{31}{OLS\relax }{figure.caption.25}{}}
\newlabel{fig:best_ridge_real}{{22b}{31}{Ridge\relax }{figure.caption.25}{}}
\newlabel{sub@fig:best_ridge_real}{{b}{31}{Ridge\relax }{figure.caption.25}{}}
\newlabel{fig:best_lasso_real}{{22c}{31}{Lasso\relax }{figure.caption.25}{}}
\newlabel{sub@fig:best_lasso_real}{{c}{31}{Lasso\relax }{figure.caption.25}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Heatmap of MSE for different choices of polynomial degree and $\lambda $. Here we have used cross validation with 5$k$-folds for data of indexes [100:141] where every second index is skipped\relax }}{31}{figure.caption.25}\protected@file@percent }
\newlabel{fig:heat_real}{{22}{31}{Heatmap of MSE for different choices of polynomial degree and $\lambda $. Here we have used cross validation with 5$k$-folds for data of indexes [100:141] where every second index is skipped\relax }{figure.caption.25}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Table of best parameters and the following MSE for OLS, Ridge and Lasso regression\relax }}{32}{table.caption.26}\protected@file@percent }
\newlabel{tab:best_comp_real}{{3}{32}{Table of best parameters and the following MSE for OLS, Ridge and Lasso regression\relax }{table.caption.26}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Predictions}{32}{subsubsection.3.3.1}\protected@file@percent }
\newlabel{fig:real_pred_real_skip}{{23a}{33}{Full data before split\relax }{figure.caption.27}{}}
\newlabel{sub@fig:real_pred_real_skip}{{a}{33}{Full data before split\relax }{figure.caption.27}{}}
\newlabel{fig:real_pred_train_skip}{{23b}{33}{Train data from function data with added noise\relax }{figure.caption.27}{}}
\newlabel{sub@fig:real_pred_train_skip}{{b}{33}{Train data from function data with added noise\relax }{figure.caption.27}{}}
\newlabel{fig:real_pred_test_skip}{{23c}{33}{Test data from function data with added noise\relax }{figure.caption.27}{}}
\newlabel{sub@fig:real_pred_test_skip}{{c}{33}{Test data from function data with added noise\relax }{figure.caption.27}{}}
\newlabel{fig:real_pred_ols_skip}{{23d}{33}{OLS prediction\relax }{figure.caption.27}{}}
\newlabel{sub@fig:real_pred_ols_skip}{{d}{33}{OLS prediction\relax }{figure.caption.27}{}}
\newlabel{fig:real_pred_ridge_skip}{{23e}{33}{Ridge prediction\relax }{figure.caption.27}{}}
\newlabel{sub@fig:real_pred_ridge_skip}{{e}{33}{Ridge prediction\relax }{figure.caption.27}{}}
\newlabel{fig:real_pred_lasos_skip}{{23f}{33}{Lasso prediction\relax }{figure.caption.27}{}}
\newlabel{sub@fig:real_pred_lasos_skip}{{f}{33}{Lasso prediction\relax }{figure.caption.27}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Plot of our data together with our train and test data, and our predictions for our optimal parameters shown in table \ref  {tab:best_comp_real} \relax }}{33}{figure.caption.27}\protected@file@percent }
\newlabel{fig:pred_real_skip}{{23}{33}{Plot of our data together with our train and test data, and our predictions for our optimal parameters shown in table \ref {tab:best_comp_real} \relax }{figure.caption.27}{}}
\newlabel{fig:real_pred_real}{{24a}{34}{Full data before split\relax }{figure.caption.28}{}}
\newlabel{sub@fig:real_pred_real}{{a}{34}{Full data before split\relax }{figure.caption.28}{}}
\newlabel{fig:real_pred_train}{{24b}{34}{Train data from function data with added noise\relax }{figure.caption.28}{}}
\newlabel{sub@fig:real_pred_train}{{b}{34}{Train data from function data with added noise\relax }{figure.caption.28}{}}
\newlabel{fig:real_pred_test}{{24c}{34}{Test data from function data with added noise\relax }{figure.caption.28}{}}
\newlabel{sub@fig:real_pred_test}{{c}{34}{Test data from function data with added noise\relax }{figure.caption.28}{}}
\newlabel{fig:real_pred_ols}{{24d}{34}{OLS prediction\relax }{figure.caption.28}{}}
\newlabel{sub@fig:real_pred_ols}{{d}{34}{OLS prediction\relax }{figure.caption.28}{}}
\newlabel{fig:real_pred_ridge}{{24e}{34}{Ridge prediction\relax }{figure.caption.28}{}}
\newlabel{sub@fig:real_pred_ridge}{{e}{34}{Ridge prediction\relax }{figure.caption.28}{}}
\newlabel{fig:real_pred_lasso}{{24f}{34}{Lasso prediction\relax }{figure.caption.28}{}}
\newlabel{sub@fig:real_pred_lasso}{{f}{34}{Lasso prediction\relax }{figure.caption.28}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Plot of our data together with our train and test data, and our predictions for our optimal parameters shown in table \ref  {tab:best_comp_real}. Here is our datasize larger than what shown in figure \ref  {fig:pred_real_skip} but from the same source to give a better visual understanding of our predictions.\relax }}{34}{figure.caption.28}\protected@file@percent }
\newlabel{fig:pred_real}{{24}{34}{Plot of our data together with our train and test data, and our predictions for our optimal parameters shown in table \ref {tab:best_comp_real}. Here is our datasize larger than what shown in figure \ref {fig:pred_real_skip} but from the same source to give a better visual understanding of our predictions.\relax }{figure.caption.28}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{35}{section.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Scaling of data}{35}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Splitting of data and overfitting}{35}{subsection.4.2}\protected@file@percent }
\abx@aux@cite{overfitting}
\abx@aux@segm{0}{0}{overfitting}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Comparison of bootstrap and cross validation}{36}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Choice of $\lambda $ parameters}{36}{subsection.4.4}\protected@file@percent }
\abx@aux@cite{sealevel}
\abx@aux@segm{0}{0}{sealevel}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Predictions}{37}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{38}{section.5}\protected@file@percent }
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{overfitting}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{cv}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{sealevel}{nty/global//global/global}
