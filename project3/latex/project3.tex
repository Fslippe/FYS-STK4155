\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{verbatim}
\usepackage{float}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage[english]{babel}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{biblatex}
%\addbibresource{project2.bib}
\usepackage{setspace}
\usepackage{wrapfig}
\usepackage{lipsum}
\usepackage{amssymb}
\linespread{1.15}
\usepackage[a4paper, total={6.5in, 9in}]{geometry}


\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}
\title{Project 3\\ }
\author{Filip Severin von der Lippe}
\date{\today}
\begin{document}
\maketitle
GitHub repository containing code and further instructions on how to reproduce the results of this report: \url{https://github.com/Fslippe/FYS-STK4155/tree/main/project3}
\url{https://openarchive.usn.no/usn-xmlui/handle/11250/2581934}
\url{https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package}
\begin{abstract}
\end{abstract}
\newpage
\tableofcontents
\newpage
\section{Introduction}
\section{Method}
\subsection{Structure of dataset} % (fold)
\label{sub:Structure of dataset}

The dataset found at \url{https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package} includes about 10 years of weather observations from numerous Australian weather stations. The observations include features such as temperature, wind, rainfall, and sunshine hours. A full overview of the dataset and its features can be found in table \ref{tab:features} in appendix \ref{app:dataset}. The main purpose of the dataset is to make a prediction based on today's weather if it is going to rain or not tomorrow. This prediction is either Yes or No, and we are therefore looking at a classification problem.



\subsection{Initializing of dataset}
The initializing and importing of the datasets require a few steps before we can train our models. These include importing the dataset to a pandas dataframe and removing all rows (days) which has at least one measurement missing. This allows for an easy way to load a training set to our models by excluding Not a Number (NaN) values and their possible negative influence on our models. This leaves us with less data to work with, but still a large amount.

The dataset comes with wind directions given in a 16-wind compass rose as seen in figure \ref{fig:compass}. This may cause some problems for some neural networks or other methods not built to handle letters or words as input. To solve this problem we translate them to labels from 0 to 15 going clockwise from North (0) to North North-West (15) as seen in table \ref{tab:compass}. The location of the weather station is given in names which also may cause problems for some models. This is solved by restricting each train and test sample to either a specific weather station or specific indexes corresponding to certain locations.
\begin{figure}[H]
    \centering
    \includegraphics[width=.3\textwidth]{../figures/Brosen_windrose.png}
    \caption{16-wind compass rose used to describe wind directions. Labels are defined as N (North), S (South), W (West) and E (East).}
    \label{fig:compass}
\end{figure}
\begin{table}[H]
    \begin{small}
        \caption{Translation and labeling of the different wind directions into integers from 0 to 15.}
        \label{tab:compass}
        \begin{center}
            \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
                \hline
                N & NNE & NE & ENE & E & ESE & SE & SSE & S & SSW & SW & WSW & W  & WNW & NW & NNW \\
                \hline
                0 & 1   & 2  & 3   & 4 & 5   & 6  & 7   & 8 & 9   & 10 & 11  & 12 & 13  & 14 & 15  \\
                \hline
            \end{tabular}
        \end{center}
    \end{small}
\end{table}

\subsection{Choosing data to analyze} % (fold)
\label{sub:Weather stations}
For our testing of the models and the dataset we start by looking at one single weather station Cobar. The data from this weather station containts 534 days which makes it one of the smaller datasets, enabling fast testing of our model. For further analysis we mainly look into three different weather stations. The choices are based on geographic position in hopes of finding a trained model's location dependency as well as its flexibility. We again look at the station Cobar, the station Coffs Harbour slightly to the east, and the north most weather station Darwin. These locations are highlighted below in figure \ref{fig:earth}. We also look into the dataset as a whole combining all the weather stations marked in figure \ref{fig:earth}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{../figures/earth.png}
    \caption{Satellite image of Australia with all weather stations marked with blue points. The three stations Cobar, Darwin and Coffs Harbour which we will focus on are marked with larger points. \textit{Picture source: Screenshot from Google Earth dated 12. Dec. 2022.}}
    \label{fig:earth}
\end{figure}
% subsubsection Weather stations (end)

\subsection{Implementation of methods} % (fold)
\label{sub:Implementation of methods}

\subsubsection*{Activation and cost functions}
To analyze binary data we need to define a cost function giving a way for our models to adjust their parameters in order to make better predictions. By starting with the logistic regression function for a prediction $\hat{y}_i$ taking values of either 1 or 0
\begin{align*}
    p(t) = \frac{e^{\hat{y}_i}}{1 + e^{\hat{y}}},
\end{align*}
one can as seen in appendix \ref{app:NN} derive the binary cross entropy function
\begin{align*}
    C(\theta) = - \sum_{i=1}^n (y_i\hat{y_i} - \log[1+ e^{\hat{y}_i}]).
\end{align*}

To implement a Neural Network we need to define an activation function. For our hidden layers we implement the ReLU function. This is defined as
\begin{align*}
    f(x) = max(0, x) \quad\quad \frac{d f }{dx} = \begin{cases}
        0 & x \leq 0 \\
        1 & x > 0
    \end{cases}
\end{align*}
Since we are dealing with a classification problem we need to restrict our predictions to 0 and 1. This can be done with help from the sigmoid activation function for the output layer.
\begin{align*}
    f(x) = \frac{1 }{1 + e^{-x}} \quad\quad \frac{df }{dx} = f(x)(1-f(x))
\end{align*}
The function restricts all output $\hat{y}$ to the interval [0,1] which can easily be translated to either 0 or 1 by
\begin{align*}
    \hat{y} =
    \begin{cases}
        0 & \text{if } \hat{y} < 0.5    \\
        1 & \text{if } \hat{y} \geq 0.5
    \end{cases}
\end{align*}
\subsubsection*{Gradient Descent and Logistic Regression}
In order to perform Logistic Regression we implement Gradient descent. We will look at Stochastic Gradient Decent (SGD) which are based on minimizing some cost function to reduce the error of predictions. This is done by first splitting up input and target data in $M$ minibatches $B_j$ denoted as $x_i$ and $y_i$. The gradient of the cost function is then computed for each minibatch which gives us $M$ different contributions to the total gradient. Each of the contributions are multiplied with the step size $\eta_j$ at the given iteration. This stepping process can be written as
\begin{align*}
    \theta_{j+1} = \theta_j - \eta_j \sum_{i \in B_k}^M \nabla_\theta C(x_i, \theta_j).
\end{align*}
There are multiple ways of optimizing the stepping process of SGD. Two of the best performers depending on the dataset are ADAM and plain SGD with momentum. By adding momentum we can help the gradient decent to gain speed in regions of small gradients at the same time as supressing oscillations in high curvature regions. This leads to a reduced risk ending up in a local minimum and can greatly speed up the training process. This is implemented for SGD in terms of the momentum $\lambda$ as
\begin{align*}
    v_j = \lambda v_{j-1}  - \eta_j \nabla_\theta C(\theta_j) \quad\quad \theta_{j+1} = \theta_j +  \sum_{i \in B_k}^Mv_j.
\end{align*}
The ADAM optimizer for makes use of momentum as well as a gradual reduction in the total stepping process as a function of the last calculated gradients. This is done by accumulating and keeping track of the squared gradient $g_{j,i}^2$. This can greatly reduce the stepping when a minimum is found, reducing oscillations giving more accurate predictions. The optimization is written for SGD with $\rho_2=0.99$ and $\rho_1=0.9$ as follows.
\begin{align*}
    m_j = \rho_1 m_{j-1} + (1-\rho_1)\nabla_\theta C(x_i, \theta_k) \quad\quad s_j = \rho_2 s_{j-1} + (1- \rho_2)g_{j,i}^2
\end{align*}
\begin{align*}
    \hat{m}_j = \frac{m_j }{1-\rho_1^t} \quad\quad \hat{s}_j = \frac{s_j }{1- \rho_2^t}
\end{align*}
\begin{align*}
    \theta_{j+1} = \theta_j - \eta_j\sum_{i \in B_k}^M \frac{\hat{m}_j}{\sqrt{\hat{s}_j} + \epsilon }
\end{align*}
Here we have the accumulating gradient for each iteration $j$ and minibatch $i$ defined as:
\begin{align*}
    g_{j, i}^2 = \sum_{i \in B_k}^M(\nabla_\theta C(x_i, \theta_j))^2
\end{align*}
\subsubsection*{Neural Network}
A Neural Network makes use of SGD and ADAM as described above, and are built up by layers of neurons. These layers can be split up in one input layer, $n$ number of hidden layer, and one output layer. The input layer contains one neuron for every feature of the input data, each connected to every neuron of the first hidden layer. The hidden layers can have any chosen amount of neurons each connected to every neuron of the next layer. The output layer contains one neuron per target category which for our classification case is just 1.

The connection and belonging activation of each neuron is dependent on a set of weights and biases that are optimized in order to best fit the input target data. For the TensorFlow Neural Network used, the biases are initially set to 0, and the weights randomly chosen from a normal distribution with mean 0 and standard deviation 0.05 \cite{tf}. For every iteration the Neural Network runs through a feed forward process. This process starts by scaling every neuron's input before it is sent to the next layer of neurons. This scaling is dependent by each neuron connection's belonging weight and bias, where the weight is multiplied and the bias added to the input. At the next layer the scaled input is fed to the layer's activation function (ReLU) before it is again scaled and sent to the next layer. When arriving at the output layer the input is directly sent to the output layer's activation function (sigmoid) which corresponds to a prediction. In order to make better predictions the weights and biases for all the neuron connections are adjusted through backpropagation. This is done by first calculating the error of the output layer with help from the cost function (binary cross entropy) before using the output layer's error to calculate the error for every layer before.  Through SGD with or without ADAM the weights and biases for every layer are updated in order to minimize the error of that layer. A full derivation of the backpropagation process can be found in appendix \ref{app:NN}.
\subsubsection*{Random Forest}


% subsection Implementation of methods (end)



\section{Results}
To get a better understanding of the dataset we plot the correlation matrix as seen in figure \ref{fig:corr}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/correlation_heatmap_full.png}
    \caption{Correlation between the different features of the dataset.}
    \label{fig:corr}
\end{figure}

In our case of mainly looking at how today's weather influences tomorrow's weather is the bottom row or the rightmost column in figure \ref{fig:corr} what interests us. We see that the different features influences the feature "RainTomorrow" to different degrees. Number of sunshine hours is the feature with the most negative correlation while the humidity at 3pm is the feature with the most positive correlation.

In figure \ref{fig:features} we see how different weather stations deviate from the mean. We see sunshine hours in \ref{fig:sunshine} and humidity in \ref{fig:humidity}
\begin{figure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=.8\textwidth]{../figures/Sunshine.png}
        \caption{}
        \label{fig:sunshine}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=.8\textwidth]{../figures/Relative_humidity3pm.png}
        \caption{}
        \label{fig:humidity}
    \end{subfigure}
    \caption{}
    \label{fig:features}
\end{figure}



Our first tests include a layer-neurons grid search for the Neural Network and a trees-depth grid search for the Random Forest. For the Neural Network and the logistic regression we perform the grid search using both ADAM and Stochastic Gradient Decent as optimizers for the gradient decent as seen in figure \ref{fig:cobar_grid}. Similarly, we see a Random Forest grid search in figure \ref{fig:cobar_grid_rf}.
\begin{figure}[H]
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/NN_grid_SGD_bootstrap_cobar.png}
        \caption{SGD}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/NN_grid_ADAM_bootstrap_cobar.png}
        \caption{ADAM}
        \label{fig:}
    \end{subfigure}
    \caption{Grid search for different learning rates ($\eta$) and L-2 norm ($\lambda$) for logistic regression. Momentum of 0.3, a batch size of 32 together with 100 epochs has been used.}
    \label{fig:cobar_grid}
\end{figure}
\begin{figure}[H]
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/logreg_momentum_Cobar.png}
        \caption{SGD}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/logreg_ADAM_Cobar.png}
        \caption{ADAM}
        \label{fig:}
    \end{subfigure}
    \caption{Grid search using 10 bootstrap iterations for different number of neurons and hidden layers in a tensorflow sequential Neural Network. ReLU has been used as activation for the hidden layers, and sigmoid for the output layer. Binary crossentropy has been used as loss function. A batch size of 32 together with 100 epochs has been used. All other parameters have been automatically chosen by tensorflow.}
    \label{fig:cobar_grid}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{../figures/RF_grid_bootstrap_cobar.png}
    \caption{Grid search using 10 bootstrap iterations for different number and depth of trees for a tensorflow Random Forest model. All other parameters are chosen automatically by tensorflow}
    \label{fig:cobar_grid_rf}
\end{figure}

We see that the performance in both figure \ref{fig:cobar_grid} and \ref{fig:cobar_grid_rf} vary depending on the parameters tested. The Neural Network performs slightly better than the Random Forest. The optimal parameters for the two models can be found below in table \ref{tab:grid_NN} and \ref{tab:grid_RF}.
\begin{table}[H]
    \caption{Optimal parameters found in grid search for Neural Network. 100 epochs and as batch size of 32 has been used. Other parameters are automatically chosen by tensorflow}
    \label{tab:grid_NN}
    \centering
    \begin{tabular}{|l|l|l|l|l|}
        \hline
        \textbf{Train Data}      & \textbf{Test Data}         & \textbf{Neurons} & \textbf{Layers} & \textbf{Accuracy} \\
        \hline
        80\% split of Cobar data & 20\% split of Cobar data   & 20               & 2               & 97.2\%            \\
        \hline
        100\% of Cobar data      & 100\% of CoffsHarbour data & 20               & 2               & 74.5\%            \\
        \hline
        100\% of Cobar data      & 100\% of Darwin data       & 20               & 2               & 75.5\%            \\
        \hline
        80\% split of all data   & 20\% split of all data     & 50               & 4               & 85.7\%            \\
        \hline
        80\% of full data        & 20\% of Cobar data         & 50               & 4               & 92.5\%            \\
        \hline
        80\% of full data        & 20\% of Darwin data        & 50               & 4               & 85.3\%            \\
        \hline
    \end{tabular}
\end{table}

\subsection{Full data analysis} % (fold)
\label{sub:Full data analysis}
We look at a dataset containing all weather stations and perform a grid search for the random forest as shown in figure \ref{fig:grid_full}.
\begin{figure}[H]
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/RF_grid_all.png}
        \caption{Random Forest}
        \label{fig:rf_full}
    \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/NN_grid_ADAM_bootstrap_all.png}
        \caption{Neural Network with ADAM}
        \label{fig:NN_full}
    \end{subfigure}
    \caption{Grid search using 10 bootstrap iterations for a Neural Network and a Random Forest. For the Neural Network has ReLU been used as activation for the hidden layers, and sigmoid for the output layer. Binary crossentropy has been used as loss function. A batch size of 320 together with 100 epochs has been used. All other parameters for both the Neural Network and the Random Forest have been automatically chosen by tensorflow.}
    \label{fig:grid_full}
\end{figure}

We see in \ref{fig:rf_full} that more trees and a larger depth generally equals a more accurate model. For the Neural Network wee see more variation and not a clear tendency with more neurons and layers equalling a more accurate model. The parameters used, and the highest accuracy is shown in table \ref{tab:grid_RF}.

\begin{table}[H]
    \caption{Optimal parameters found in grid search for Random Forest. Other parameters are chosen automatically by tensorflow.}
    \label{tab:grid_RF}
    \centering
    \begin{tabular}{|l|l|l|l|l|}
        \hline
        \textbf{Train Data}      & \textbf{Test Data}         & \textbf{Trees} & \textbf{Depth} & \textbf{Accuracy} \\
        \hline
        80\% split of Cobar data & 20\% split of Cobar data   & 100            & 10             & 96.3\%            \\
        \hline
        80\% split of all data   & 20\% split of all data     & 500            & 20             & 86.6\%            \\
        \hline
        100\% of Cobar data      & 100\% of CoffsHarbour data & 100            & 10             & 77.9\%            \\
        \hline
        100\% of Cobar data      & 100\% of Darwin data       & 100            & 10             & 82.4\%            \\
        \hline
        80\% of full data        & 20\% of Cobar data         & 500            & 20             & 94.4\%            \\
        \hline
        80\% of full data        & 20\% of Darwin data        & 500            & 20             & 87.4\%            \\
        \hline
    \end{tabular}
\end{table}
% subsection Full data analysis (end)
\begin{itemize}
    \item initializing a pandas dataframe,
    \item remove all rows with at least one measurement missing,
    \item Look at sized of data from the different stations - spanning from 534 to 3062 for 22 features
    \item will need package tensorflowdecisionforests
    \item Bootstrap to reduce overfitting and improve generalization ability
    \item perform bootstrap on different weight and bias initializations
    \item Current small scale weather stations - lots of data
    \item Compairison between areas - assume that masked values not happen at specific times and weather conditions
    \item Relative high accuracy on Cobar test data - Climate maybe largest factor not weather station.  - train model on data on the other side of the world - help weather predictions here?
\end{itemize}


\newpage
\appendix
\section{Dataset}
\label{app:dataset}
\begin{table}[H]
    \begin{small}
        \caption{Description of dataset features to predict the last feature "RainTomorrow" if it is going to rain tomorrow or not }
        \label{tab:features}
        \begin{center}
            \begin{tabular}{|l|l|l|}
                \hline
                \textbf{Feature} & \textbf{desctiption}                                 & \textbf{Unit}           \\
                \hline
                \hline
                Location         & Common name of the weather station                   & name                    \\
                \hline
                MinTemp          & Minimum temperature                                  & degrees Celcius         \\
                \hline
                MaxTemp          & Maximum temperature                                  & degrees Celcius         \\
                \hline
                Rainfall         & Amount of rainfall recorded in the day               & mm                      \\
                \hline
                Evaporation      & The "Class A" pan evaporation in the 24 hours        & mm                      \\
                \hline
                Sunshine         & Number of hours with bright sunshine in the day      & hours                   \\
                \hline
                WindGustDir      & direction of the strongest wind gust in the 24 hours & 16-wind compass rose    \\
                \hline
                WindGustSpeed    & Speed of the strongest wind gust in the 24 hours     & km/h                    \\
                \hline
                WindDir9am       & wind direction at 9am                                & 16-wind compass rose    \\
                \hline
                WindDir3pm       & wind direction at 3pm                                & 16-wind compass rose    \\
                \hline
                WindSpeed9am     & Wind speed at 9am                                    & km/h                    \\
                \hline
                WindSpeed3pm     & Wind speed at 3pm                                    & km/h                    \\
                \hline
                Humidity9am      & Relative humidity at 9am                             & percent                 \\
                \hline
                Humidity3pm      & Relative humidity at 3pm                             & percent                 \\
                \hline
                Pressure9am      & Pressure reduced to mean sea level at 9am            & hPa                     \\
                \hline
                Pressure3pm      & Pressure reduced to mean sea level at 3pm            & hPa                     \\
                \hline
                Cloud9am         & Fraction of sky covered by clouds at 9am             & oktas (units of eights) \\
                \hline
                Cloud3pm         & Fraction of sky covered by clouds at 3pm             & oktas (units of eights) \\
                \hline
                Temp9am          & Temperature at 9am                                   & degrees Celcius         \\
                \hline
                Temp3pm          & Temperature at 3pm                                   & degrees Celcius         \\
                \hline
                RainToday        & Rain exceeding 1mm over 24 hours today               & Yes or No               \\
                \hline
                RainTomorrow     & Rain exceeding 1mm over 24 hours tomorrow            & Yes or No               \\
                \hline
            \end{tabular}
        \end{center}
    \end{small}
\end{table}

\section{Neural Network and Cost function}
\label{app:NN}
The following derivations are taken from \cite{project2}
\subsubsection*{Cross Entropy}
To analyze binary data where possible solutions are $y_i=0$ and $y_i=1$ we define the logistic function:
\begin{align}
    \label{eq:logistic}
    p(t) =  \frac{e^{\hat{y}_i}}{1+e^{\hat{y}_i}}
\end{align}
Where $\hat{y}_i$ is defined by:
\begin{align*}
    \hat{y}_i = \theta_0 + \theta_1 x_i +...+ \theta_n x_i^n
\end{align*}
To find a cost function and following gradient we use Maximum Likelihood Estimation from the dataset $\mathcal{D} \in \{x_i, y_i\}$:
\begin{align*}
    p(\mathcal{D}|\boldsymbol{\theta}) = \prod_{i=1}^n (p(y_i = 1|x_i,\boldsymbol{\theta}))^{y_i}\left( 1- p(y_i = 1 | x_i, \boldsymbol{\theta})\right)^{1-y_i}
\end{align*}
where we have used:
\begin{align*}
    p(y_i=0|x_i, \boldsymbol{\theta} ) = 1 - p(y_i=1 | x_i, \boldsymbol{\theta})
\end{align*}
giving us the log-likelihood and our cost function:
\begin{align*}
    C(\boldsymbol{\theta}) = \sum_{i=1}^n (y_i \log p(y_i =1 | x_i, \boldsymbol{\theta})) + (1- y_i) \log [1 - p(y_i \log p(y_i =1 | x_i, \boldsymbol{\theta}))]
\end{align*}
Recognizing the log-likelihood being a maximizing function we use the negative log-likelihood which rewritten gives us:
\begin{align}
    \label{eq:cross}
    C(\boldsymbol{\theta}) = -\sum_{i=1}^n (y_i \hat{y}_i - \log [1 + e^{\hat{y}_i}])
\end{align}
\subsubsection*{Backpropagation}
The error found using the Neural Network's cost function does not directly say anything on how to update the weights ($w_{ij}^{L}$) and biases ($b_{j}^{L}$) for layer $L$, neuron $j$ and connection $ij$ between neuron $i$ of the last layer and neuron $j$. To find this we can use the definition of the prediction made by the Neural Network. This prediction $\hat{y}$ is dependent on the value $a_l$ which comes from the value $z_l$ through the layer's activation function. To apply the cost function on the weight and biases we use the chain rule for layer $L$:
\begin{align*}
    \frac{\partial C}{\partial w^{(L)}_{ij}}  = \frac{\partial z^{(L)}_j}{\partial w^{(L)}_{ij}}  \frac{\partial a^{(L)}_{j}}{\partial z^{(L)}_j}  \frac{\partial C}{\partial a^{(L)}_j}
\end{align*}
rewriting in terms of the activation function for simplicity here denoted as the sigmoid $\sigma$, and using the calculation of $z$ throughout the layers:
\begin{align*}
    a_j^L = \sigma(z_j^{(L)}) \quad\quad z_j^{(L)} = \sum_{i=1}^{n_{L-1}}w^{(L)}_{ij}a{(L-1)}_{i} + b{(L)}_{j}
\end{align*}
giving us:
\begin{align*}
    \frac{\partial C}{\partial w^{(L)}_{ij}} = \frac{\partial C}{\partial a^{(L)}_{j}}\sigma(z_j^{(L)})a_i^{(L-1)}
\end{align*}
Similarly for the bias we get:
\begin{align*}
    \frac{\partial C}{\partial b^{(L)}_{j}} = \frac{\partial z^{(L)}_j}{\partial b^{(L)}_{j}}  \frac{\partial a^{(L)}_{j}}{\partial z^{(L)}_j}  \frac{\partial C}{\partial a^{(L)}_j} =
    \frac{\partial C}{\partial a^{(L)}_{j}}\sigma(z_j^{(L)}).
\end{align*}
We define a local gradient (error $E$) as:
\begin{align*}
    E_j^{(L)} = \frac{\partial C }{\partial z_j^{(L)}} = \frac{\partial C }{\partial a_j^{(L)}}\frac{\partial a_j^{(L)} }{\partial z_j^{(L)}} = \frac{\partial C }{\partial a_j^{(L)}} \sigma(z_j^{(L)})
\end{align*}
which gives us:
\begin{align*}
    \frac{\partial C }{\partial w_{ij}^{(L)}} = E_j^{(L)} a_i^{(L-1)} \quad\quad
    \frac{\partial C }{\partial b_{j}^{(L)}} = E_j^{(L)}
\end{align*}
and in terms of matrices:
\begin{align*}
    E^{(L)} = \nabla_a C \odot \frac{\partial \sigma }{\partial z^{(L)}} \quad \text{for} \quad \nabla_a C = \left(\frac{\partial C }{\partial a_1^{(L)}}, \frac{\partial C }{\partial a_2^{(L)}},..., \frac{\partial C }{\partial a_{n_L}^{(L)}}\right).
\end{align*}
By using the chain rule we arrive at an expression for the error of one node at one layer dependent on a sum of all the errors of the next layer with neurons denoted $k$:
\begin{align*}
    E^{(l-1)}_j = \sum_k E^{(l)}_k w_{jk}^{(l)}\sigma(z_j^{(l-1)})
\end{align*}
This means that we can calculate the error and following bias and weight gradients of one layer if we know the error at the last layer. This gives us a way of updating all the weights and biases in order to reduce the error of the Neural Network's predictions. This is given by:
\begin{enumerate}
    \item calculating the error at the last layer $L$:
          \begin{align*}
              \boldsymbol{E}^{(L)} = \frac{\partial C }{\partial \boldsymbol{a}^{(L)}} \sigma(\boldsymbol{z}^{(L)})
          \end{align*}
    \item Using this to calculate the error of the other layers starting at $l=L-1$:
          \begin{align*}
              \text{for}\quad  l=L-1, L-2,...,1 \quad\quad
              E_j^{(l)} = \sum_k E^{(l+1)}_k w_{jk}^{(l+1)} \sigma(z_j^{(l)})
          \end{align*}
\end{enumerate}
We then update the weights and biases for a chosen learning rate $\eta$ and L2 norm $\lambda$:
\begin{align*}
    w^{(l)}_{ij} = w^{(l)}_{ij} - \eta(E_j^{(l-1)}  + \lambda w_{ij}^{(l)}) \quad\quad b_j^{(l)} = b_j^{(l)} - \eta(E_j^{(l)} + \lambda b_j^{(l)})
\end{align*}
\end{document}